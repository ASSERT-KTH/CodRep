final Version version = Lucene.parseVersionLenient(info.info.getVersion(), Version.LUCENE_3_0);

/*
 * Licensed to Elasticsearch under one or more contributor
 * license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright
 * ownership. Elasticsearch licenses this file to you under
 * the Apache License, Version 2.0 (the "License"); you may
 * not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.elasticsearch.index.store;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Iterables;
import org.apache.lucene.codecs.CodecUtil;
import org.apache.lucene.index.*;
import org.apache.lucene.store.*;
import org.apache.lucene.util.IOUtils;
import org.apache.lucene.util.Version;
import org.elasticsearch.ExceptionsHelper;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.Strings;
import org.elasticsearch.common.compress.Compressor;
import org.elasticsearch.common.compress.CompressorFactory;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.lucene.Directories;
import org.elasticsearch.common.lucene.Lucene;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.CloseableIndexComponent;
import org.elasticsearch.index.codec.CodecService;
import org.elasticsearch.index.settings.IndexSettings;
import org.elasticsearch.index.shard.AbstractIndexShardComponent;
import org.elasticsearch.index.shard.ShardId;
import org.elasticsearch.index.store.distributor.Distributor;
import org.elasticsearch.index.store.support.ForceSyncDirectory;

import java.io.*;
import java.nio.file.NoSuchFileException;
import java.util.*;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * A Store provides plain access to files written by an elasticsearch index shard. Each shard
 * has a dedicated store that is uses to access lucenes Directory which represents the lowest level
 * of file abstraction in lucene used to read and write Lucene indices to.
 * This class also provides access to metadata information like checksums for committed files. A committed
 * file is a file that belongs to a segment written by a Lucene commit. Files that have not been committed
 * ie. created during a merge or a shard refresh / NRT reopen are not considered in the MetadataSnapshot.
 *
 * Note: If you use a store it's reference count should be increased before using it by calling #incRef and a
 * corresponding #decRef must be called in a try/finally block to release the store again ie.:
 * <pre>
 *      store.incRef();
 *      try {
 *        // use the store...
 *
 *      } finally {
 *          store.decRef();
 *      }
 * </pre>
 */
public class Store extends AbstractIndexShardComponent implements CloseableIndexComponent, Closeable {

    private static final String CODEC = "store";
    private static final int VERSION = 0;
    private static final String CORRUPTED = "corrupted_";

    private final AtomicBoolean isClosed = new AtomicBoolean(false);
    private final AtomicInteger refCount = new AtomicInteger(1);
    private final IndexStore indexStore;
    private final CodecService codecService;
    private final DirectoryService directoryService;
    private final StoreDirectory directory;
    private final boolean sync;
    private final DistributorDirectory distributorDirectory;

    @Inject
    public Store(ShardId shardId, @IndexSettings Settings indexSettings, IndexStore indexStore, CodecService codecService, DirectoryService directoryService, Distributor distributor) throws IOException {
        super(shardId, indexSettings);
        this.indexStore = indexStore;
        this.codecService = codecService;
        this.directoryService = directoryService;
        this.sync = componentSettings.getAsBoolean("sync", true);
        this.distributorDirectory = new DistributorDirectory(distributor);
        this.directory = new StoreDirectory(distributorDirectory);
    }

    public IndexStore indexStore() {
        ensureOpen();
        return this.indexStore;
    }

    public Directory directory() {
        ensureOpen();
        return directory;
    }

    /**
     * Returns the last committed segments info for this store
     * @throws IOException if the index is corrupted or the segments file is not present
     */
    public SegmentInfos readLastCommittedSegmentsInfo() throws IOException {
        return readLastCommittedSegmentsInfo(directory());
    }

    /**
     * Returns the last committed segments info for the given directory
     * @throws IOException if the index is corrupted or the segments file is not present
     */
    private static SegmentInfos readLastCommittedSegmentsInfo(Directory directory) throws IOException {
        try {
            return Lucene.readSegmentInfos(directory);
        } catch (EOFException eof) {
            // TODO this should be caught by lucene - EOF is almost certainly an index corruption
            throw new CorruptIndexException("Read past EOF while reading segment infos", eof);
        }
    }

    private final void ensureOpen() {
        if (this.refCount.get() <= 0) {
            throw new AlreadyClosedException("Store is already closed");
        }
    }

    /**
     * Returns a new MetadataSnapshot.
     */
    public MetadataSnapshot getMetadata() throws IOException {
        ensureOpen();
        failIfCorrupted();
        try {
            return new MetadataSnapshot(distributorDirectory, logger);
        } catch (CorruptIndexException ex) {
            markStoreCorrupted(ex);
            throw ex;
        }
    }

    /**
     * Deletes the content of a shard store. Be careful calling this!.
     */
    public void deleteContent() throws IOException {
        ensureOpen();
        final String[] files = distributorDirectory.listAll();
        IOException lastException = null;
        for (String file : files) {
            try {
                distributorDirectory.deleteFile(file);
            } catch (NoSuchFileException | FileNotFoundException e) {
                // ignore
            } catch (IOException e) {
                lastException = e;
            }
        }
        if (lastException != null) {
            throw lastException;
        }
    }

    public StoreStats stats() throws IOException {
        ensureOpen();
        return new StoreStats(Directories.estimateSize(directory), directoryService.throttleTimeInNanos());
    }

    public void renameFile(String from, String to) throws IOException {
        ensureOpen();
        distributorDirectory.renameFile(directoryService, from, to);
    }

    /**
     * Returns <tt>true</tt> by default.
     */
    public boolean suggestUseCompoundFile() {
        return false;
    }

    /**
     * Increments the refCount of this Store instance.  RefCounts are used to determine when a
     * Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a
     * corresponding {@link #decRef}, in a finally clause; otherwise the store may never be closed.  Note that
     * {@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link
     * #decRef} has been called for all outstanding references.
     *
     * Note: Close can safely be called multiple times.
     * @see #decRef
     * @see #tryIncRef()
     * @throws AlreadyClosedException iff the reference counter can not be incremented.
     */
    public final void incRef() {
        if (tryIncRef() == false) {
            throw new AlreadyClosedException("Store is already closed can't increment refCount current count [" + refCount.get() + "]");
        }
    }

    /**
     * Tries to increment the refCount of this Store instance. This method will return <tt>true</tt> iff the refCount was
     * incremented successfully otherwise <tt>false</tt>. RefCounts are used to determine when a
     * Store can be closed safely, i.e. as soon as there are no more references. Be sure to always call a
     * corresponding {@link #decRef}, in a finally clause; otherwise the store may never be closed.  Note that
     * {@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link
     * #decRef} has been called for all outstanding references.
     *
     * Note: Close can safely be called multiple times.
     * @see #decRef()
     * @see #incRef()
     */
    public final boolean tryIncRef() {
        do {
            int i = refCount.get();
            if (i > 0) {
                if (refCount.compareAndSet(i, i + 1)) {
                    return true;
                }
            } else {
                return false;
            }
        } while (true);
    }

    /**
     * Decreases the refCount of this Store instance.If the refCount drops to 0, then this
     * store is closed.
     * @see #incRef
     */
    public final void decRef() {
        int i = refCount.decrementAndGet();
        assert i >= 0;
        if (i == 0) {
            closeInternal();
        }

    }

    @Override
    public void close() {
        if (isClosed.compareAndSet(false, true)) {
            // only do this once!
            decRef();
        }
    }

    private void closeInternal() {
        try {
            directory.innerClose(); // this closes the distributorDirectory as well
        } catch (IOException e) {
            logger.debug("failed to close directory", e);
        }
    }


    public static MetadataSnapshot readMetadataSnapshot(File[] indexLocations, ESLogger logger) throws IOException {
        final Directory[] dirs = new Directory[indexLocations.length];
        try {
            for (int i=0; i< indexLocations.length; i++) {
                dirs[i] = new SimpleFSDirectory(indexLocations[i]);
            }
            DistributorDirectory dir = new DistributorDirectory(dirs);
            failIfCorrupted(dir, new ShardId("", 1));
            return new MetadataSnapshot(dir, logger);
        } finally {
            IOUtils.close(dirs);
        }
    }

    /**
     * The returned IndexOutput might validate the files checksum if the file has been written with a newer lucene version
     * and the metadata holds the necessary information to detect that it was been written by Lucene 4.8 or newer. If it has only
     * a legacy checksum, returned IndexOutput will not verify the checksum.
     *
     * Note: Checksums are calculated nevertheless since lucene does it by default sicne version 4.8.0. This method only adds the
     * verification against the checksum in the given metadata and does not add any significant overhead.
     */
    public IndexOutput createVerifyingOutput(final String filename, final IOContext context, final StoreFileMetaData metadata) throws IOException {
        if (metadata.hasLegacyChecksum() || metadata.checksum() == null) {
            logger.debug("create legacy output for {}", filename);
            return directory().createOutput(filename, context);
        }
        assert metadata.writtenBy() != null;
        assert metadata.writtenBy().onOrAfter(Version.LUCENE_48);
        return new VerifyingIndexOutput(metadata, directory().createOutput(filename, context));
    }

    public static void verify(IndexOutput output) throws IOException {
        if (output instanceof VerifyingIndexOutput) {
            ((VerifyingIndexOutput)output).verify();
        }
    }

    public boolean checkIntegrity(StoreFileMetaData md) {
        if (md.writtenBy() != null && md.writtenBy().onOrAfter(Version.LUCENE_48)) {
            try (IndexInput input = directory().openInput(md.name(), IOContext.READONCE)) {
                CodecUtil.checksumEntireFile(input);
            } catch (IOException  e) {
                return false;
            }
        }
        return true;
    }

    public boolean isMarkedCorrupted() throws IOException {
        ensureOpen();
        /* marking a store as corrupted is basically adding a _corrupted to all
         * the files. This prevent
         */
        final String[] files = directory().listAll();
        for (String file : files) {
            if (file.startsWith(CORRUPTED)) {
                return true;
            }
        }
        return false;
    }

    public void failIfCorrupted() throws IOException {
        ensureOpen();
        failIfCorrupted(directory, shardId);
    }

    private static final void failIfCorrupted(Directory directory, ShardId shardId) throws IOException {
        final String[] files = directory.listAll();
        List<CorruptIndexException> ex = new ArrayList<>();
        for (String file : files) {
            if (file.startsWith(CORRUPTED)) {
                try(ChecksumIndexInput input = directory.openChecksumInput(file, IOContext.READONCE)) {
                    CodecUtil.checkHeader(input, CODEC, VERSION, VERSION);
                    String msg = input.readString();
                    StringBuilder builder = new StringBuilder(shardId.toString());
                    builder.append(" Corrupted index [");
                    builder.append(file).append("] caused by: ");
                    builder.append(msg);
                    ex.add(new CorruptIndexException(builder.toString()));
                    CodecUtil.checkFooter(input);
                }
            }
        }
        if (ex.isEmpty() == false) {
            ExceptionsHelper.rethrowAndSuppress(ex);
        }
    }

    /**
     * The idea of the store directory is to cache file level meta data, as well as md5 of it
     */
    public class StoreDirectory extends FilterDirectory implements ForceSyncDirectory {

        StoreDirectory(Directory delegateDirectory) throws IOException {
            super(delegateDirectory);
        }

        public ShardId shardId() {
            ensureOpen();
            return Store.this.shardId();
        }

        @Nullable
        public CodecService codecService() {
            ensureOpen();
            return Store.this.codecService;
        }

        @Override
        public IndexInput openInput(String name, IOContext context) throws IOException {
            IndexInput in = super.openInput(name, context);
            boolean success = false;
            try {
                // Only for backward comp. since we now use Lucene codec compression
                if (name.endsWith(".fdt") || name.endsWith(".tvf")) {
                    Compressor compressor = CompressorFactory.compressor(in);
                    if (compressor != null) {
                        in = compressor.indexInput(in);
                    }
                }
                success = true;
            } finally {
                if (!success) {
                    IOUtils.closeWhileHandlingException(in);
                }
            }
            return in;
        }

        @Override
        public void close() throws IOException {
            assert false : "Nobody should close this directory except of the Store itself";
        }

        @Override
        public void sync(Collection<String> names) throws IOException {
            if (sync) {
                super.sync(names);
            }
        }

        private void innerClose() throws IOException {
            super.close();
        }

        @Override
        public void forceSync(String name) throws IOException {
            sync(ImmutableList.of(name));
        }

        @Override
        public String toString() {
            return "store(" + in.toString() + ")";
        }
    }

    /**
     * Represents a snaphshot of the current directory build from the latest Lucene commit.
     * Only files that are part of the last commit are considered in this datastrucutre.
     * For backwards compatibility the snapshot might include legacy checksums that
     * are derived from a dedicated checksum file written by older elastcisearch version pre 1.3
     *
     * @see StoreFileMetaData
     */
    public final static class MetadataSnapshot implements Iterable<StoreFileMetaData> {
        private final ImmutableMap<String, StoreFileMetaData> metadata;

        MetadataSnapshot(Directory directory, ESLogger logger) throws IOException {
            metadata = buildMetadata(directory, logger);
        }

        ImmutableMap<String, StoreFileMetaData> buildMetadata(Directory directory, ESLogger logger) throws IOException {
            ImmutableMap.Builder<String, StoreFileMetaData> builder = ImmutableMap.builder();
            Map<String, String> checksumMap = readLegacyChecksums(directory);
            try {
                final SegmentInfos segmentCommitInfos;
                try {
                    segmentCommitInfos = Store.readLastCommittedSegmentsInfo(directory);
                } catch (FileNotFoundException | NoSuchFileException ex) {
                    // no segments file -- can't read metadata
                    logger.trace("Can't read segment infos", ex);
                    return ImmutableMap.of();
                }
                Version maxVersion = Version.LUCENE_3_0; // we don't know which version was used to write so we take the max version.
                Set<String> added = new HashSet<>();
                for (SegmentCommitInfo info : segmentCommitInfos) {
                    final Version version = Version.parseLeniently(info.info.getVersion());
                    if (version.onOrAfter(maxVersion)) {
                        maxVersion = version;
                    }
                    for (String file : Iterables.concat(info.info.files(), info.files())) {
                        if (!added.contains(file)) {
                            String legacyChecksum = checksumMap.get(file);
                            if (version.onOrAfter(Version.LUCENE_4_8) && legacyChecksum == null) {
                                checksumFromLuceneFile(directory, file, builder, logger, version);
                            } else {
                                builder.put(file, new StoreFileMetaData(file, directory.fileLength(file), legacyChecksum, null));
                            }
                            added.add(file);
                        }
                    }
                }
                for (String file : Arrays.asList(segmentCommitInfos.getSegmentsFileName(), IndexFileNames.SEGMENTS_GEN)) {
                    if (!added.contains(file)) {
                        try {
                            String legacyChecksum = checksumMap.get(file);
                            if (maxVersion.onOrAfter(Version.LUCENE_4_8) && legacyChecksum == null) {
                                checksumFromLuceneFile(directory, file, builder, logger, maxVersion);
                            } else {
                                builder.put(file, new StoreFileMetaData(file, directory.fileLength(file), legacyChecksum, null));
                            }
                            added.add(file);
                        } catch (FileNotFoundException | NoSuchFileException ex) {
                            if (IndexFileNames.SEGMENTS_GEN.equals(file) == false) {
                                // segments.gen is optional
                                throw ex;
                            }
                        }
                    }
                }
            } catch (CorruptIndexException ex) {
                throw ex;
            } catch (FileNotFoundException | NoSuchFileException ex) {
                // can't open index | no commit present -- we might open a snapshot index that is not fully restored?
                logger.warn("Can't open file to read checksums", ex);
                return ImmutableMap.of();
            } catch (Throwable ex) {
                try {
                    // Lucene checks the checksum after it tries to lookup the codec etc.
                    // in that case we might get only IAE or similar exceptions while we are really corrupt...
                    // TODO we should check the checksum in lucene if we hit an exception
                    Lucene.checkSegmentInfoIntegrity(directory);
                } catch (CorruptIndexException cex) {
                  cex.addSuppressed(ex);
                  throw cex;
                } catch (Throwable e) {
                    // ignore...
                }

                throw ex;
            }
            return builder.build();
        }

        static Map<String, String> readLegacyChecksums(Directory directory) throws IOException {
            synchronized (directory) {
                long lastFound = -1;
                for (String name : directory.listAll()) {
                    if (!isChecksum(name)) {
                        continue;
                    }
                    long current = Long.parseLong(name.substring(CHECKSUMS_PREFIX.length()));
                    if (current > lastFound) {
                        lastFound = current;
                    }
                }
                if (lastFound > -1) {
                    try (IndexInput indexInput = directory.openInput(CHECKSUMS_PREFIX + lastFound, IOContext.READONCE)) {
                        indexInput.readInt(); // version
                        return indexInput.readStringStringMap();
                    }
                }
                return new HashMap<>();
            }
        }

        private static void checksumFromLuceneFile(Directory directory, String file, ImmutableMap.Builder<String, StoreFileMetaData> builder,  ESLogger logger, Version version) throws IOException {
            try (IndexInput in = directory.openInput(file, IOContext.READONCE)) {
                try {
                    if (in.length() < CodecUtil.footerLength()) {
                        // truncated files trigger IAE if we seek negative... these files are really corrupted though
                        throw new CorruptIndexException("Can't retrieve checksum from file: " + file + " file length must be >= " + CodecUtil.footerLength() + " but was: " + in.length());
                    }
                    String checksum = digestToString(CodecUtil.retrieveChecksum(in));
                    builder.put(file, new StoreFileMetaData(file, directory.fileLength(file), checksum, version));
                } catch (Throwable ex) {
                    logger.debug("Can retrieve checksum from file [{}]", ex, file);
                    throw ex;
                }
            }
        }


        @Override
        public Iterator<StoreFileMetaData> iterator() {
            return metadata.values().iterator();
        }

        public StoreFileMetaData get(String name) {
            return metadata.get(name);
        }

        public Map<String, StoreFileMetaData> asMap() {
            return metadata;
        }
    }

    public final static class LegacyChecksums {
        private final Map<String, String> legacyChecksums = new HashMap<>();

        public void add(StoreFileMetaData metaData) throws IOException {

            if (metaData.hasLegacyChecksum()) {
                synchronized (this) {
                    // we don't add checksums if they were written by LUCENE_48... now we are using the build in mechanism.
                    legacyChecksums.put(metaData.name(), metaData.checksum());
                }
            }
        }

        public synchronized void write(Store store) throws IOException {
            synchronized (store.distributorDirectory) {
                Map<String, String> stringStringMap = MetadataSnapshot.readLegacyChecksums(store.distributorDirectory);
                stringStringMap.putAll(legacyChecksums);
                if (!stringStringMap.isEmpty()) {
                    writeChecksums(store.directory, stringStringMap);
                }
            }
        }

        synchronized void writeChecksums(Directory directory, Map<String, String> checksums) throws IOException {
            String checksumName = CHECKSUMS_PREFIX + System.currentTimeMillis();
            while (directory.fileExists(checksumName)) {
                checksumName = CHECKSUMS_PREFIX + System.currentTimeMillis();
            }
            try (IndexOutput output = directory.createOutput(checksumName, IOContext.DEFAULT)) {
                output.writeInt(0); // version
                output.writeStringStringMap(checksums);
            }
            directory.sync(Collections.singleton(checksumName));
        }

        public void clear() {
            this.legacyChecksums.clear();
        }

        public void remove(String name) {
            legacyChecksums.remove(name);
        }
    }

    private static final String CHECKSUMS_PREFIX = "_checksums-";

    public static final boolean isChecksum(String name) {
        // TODO can we drowp .cks
        return name.startsWith(CHECKSUMS_PREFIX) || name.endsWith(".cks"); // bwcomapt - .cks used to be a previous checksum file
    }

    /**
     * Produces a string representation of the given digest value.
     */
    public static String digestToString(long digest) {
        return Long.toString(digest, Character.MAX_RADIX);
    }


    static class VerifyingIndexOutput extends IndexOutput {

        private final StoreFileMetaData metadata;
        private final IndexOutput output;
        private long writtenBytes;
        private final long checksumPosition;
        private String actualChecksum;

        VerifyingIndexOutput(StoreFileMetaData metadata, IndexOutput actualOutput) {
            this.metadata = metadata;
            this.output = actualOutput;
            checksumPosition = metadata.length() - 8; // the last 8 bytes are the checksum
        }

        @Override
        public void flush() throws IOException {
            output.flush();
        }

        @Override
        public void close() throws IOException {
            output.close();
        }

        @Override
        public long getFilePointer() {
            return output.getFilePointer();
        }

        @Override
        public long getChecksum() throws IOException {
            return output.getChecksum();
        }

        @Override
        public long length() throws IOException {
            return output.length();
        }

        /**
         * Verifies the checksum and compares the written length with the expected file length. This method should bec
         * called after all data has been written to this output.
         */
        public void verify() throws IOException {
            if (metadata.checksum().equals(actualChecksum) && writtenBytes == metadata.length()) {
                return;
            }
            throw new CorruptIndexException("verification failed (hardware problem?) : expected=" + metadata.checksum() +
                    " actual=" + actualChecksum + " writtenLength=" + writtenBytes + " expectedLength=" + metadata.length() +
                    " (resource=" + metadata.toString() + ")");
        }

        @Override
        public void writeByte(byte b) throws IOException {
            if (writtenBytes++ == checksumPosition) {
                readAndCompareChecksum();
            }
            output.writeByte(b);
        }

        private void readAndCompareChecksum() throws IOException {
            actualChecksum = digestToString(getChecksum());
            if (!metadata.checksum().equals(actualChecksum)) {
                throw new CorruptIndexException("checksum failed (hardware problem?) : expected=" + metadata.checksum() +
                        " actual=" + actualChecksum +
                        " (resource=" + metadata.toString() + ")");
            }
        }

        @Override
        public void writeBytes(byte[] b, int offset, int length) throws IOException {
            if (writtenBytes + length > checksumPosition && actualChecksum == null) {
                assert writtenBytes <= checksumPosition;
                final int bytesToWrite = (int)(checksumPosition-writtenBytes);
                output.writeBytes(b, offset, bytesToWrite);
                readAndCompareChecksum();
                offset += bytesToWrite;
                length -= bytesToWrite;
                writtenBytes += bytesToWrite;
            }
            output.writeBytes(b, offset, length);
            writtenBytes += length;
        }

    }

    public void deleteQuiet(String... files) {
        for (String file : files) {
            try {
                directory().deleteFile(file);
            } catch (Throwable ex) {
                // ignore
            }
        }
    }

    /**
     * Marks this store as corrupted. This method writes a <tt>corrupted_${uuid}</tt> file containing the given exception
     * message. If a store contains a <tt>corrupted_${uuid}</tt> file {@link #isMarkedCorrupted()} will return <code>true</code>.
     */
    public void markStoreCorrupted(CorruptIndexException exception) throws IOException {
        ensureOpen();
        if (!isMarkedCorrupted()) {
            String uuid = CORRUPTED + Strings.randomBase64UUID();
            try(IndexOutput output = this.directory().createOutput(uuid, IOContext.DEFAULT)) {
                CodecUtil.writeHeader(output, CODEC, VERSION);
                output.writeString(ExceptionsHelper.detailedMessage(exception, true, 0)); // handles null exception
                CodecUtil.writeFooter(output);
            } catch (IOException ex) {
                logger.warn("Can't mark store as corrupted", ex);
            }
            directory().sync(Collections.singleton(uuid));
        }
    }
}