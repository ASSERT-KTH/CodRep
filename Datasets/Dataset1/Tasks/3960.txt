doAtomicUpdate();

package org.apache.solr.cloud;

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import org.apache.solr.client.solrj.SolrQuery;
import org.apache.solr.client.solrj.SolrServer;
import org.apache.solr.client.solrj.SolrServerException;
import org.apache.solr.client.solrj.embedded.JettySolrRunner;
import org.apache.solr.client.solrj.impl.CloudSolrServer;
import org.apache.solr.client.solrj.request.UpdateRequest;
import org.apache.solr.client.solrj.response.QueryResponse;
import org.apache.solr.common.SolrDocument;
import org.apache.solr.common.SolrException;
import org.apache.solr.common.SolrInputDocument;
import org.apache.solr.common.cloud.CompositeIdRouter;
import org.apache.solr.common.cloud.ZkNodeProps;
import org.apache.solr.common.cloud.ZkStateReader;
import org.apache.solr.common.params.CommonParams;
import org.apache.solr.common.params.ModifiableSolrParams;
import org.apache.solr.common.params.ShardParams;
import org.apache.solr.common.util.StrUtils;
import org.apache.solr.servlet.SolrDispatchFilter;
import org.apache.solr.update.DirectUpdateHandler2;
import org.junit.BeforeClass;
import org.junit.Ignore;

import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;


public class ShardRoutingTest extends AbstractFullDistribZkTestBase {

  String bucket1 = "shard1";      // shard1: top bits:10  80000000:bfffffff
  String bucket2 = "shard2";      // shard2: top bits:11  c0000000:ffffffff
  String bucket3 = "shard3";      // shard3: top bits:00  00000000:3fffffff
  String bucket4 = "shard4";      // shard4: top bits:01  40000000:7fffffff


  @BeforeClass
  public static void beforeShardHashingTest() throws Exception {
    // TODO: we use an fs based dir because something
    // like a ram dir will not recover correctly right now
    // because tran log will still exist on restart and ram
    // dir will not persist - perhaps translog can empty on
    // start if using an EphemeralDirectoryFactory 
    useFactory(null);
  }

  public ShardRoutingTest() {
    schemaString = "schema15.xml";      // we need a string id
    super.sliceCount = 4;
    super.shardCount = 8;
    super.fixShardCount = true;  // we only want to test with exactly 4 slices.

    // from negative to positive, the upper bits of the hash ranges should be
    // shard1: top bits:10  80000000:bfffffff
    // shard2: top bits:11  c0000000:ffffffff
    // shard3: top bits:00  00000000:3fffffff
    // shard4: top bits:01  40000000:7fffffff

    /***
     hash of a is 3c2569b2 high bits=0 shard=shard3
     hash of b is 95de7e03 high bits=2 shard=shard1
     hash of c is e132d65f high bits=3 shard=shard2
     hash of d is 27191473 high bits=0 shard=shard3
     hash of e is 656c4367 high bits=1 shard=shard4
     hash of f is 2b64883b high bits=0 shard=shard3
     hash of g is f18ae416 high bits=3 shard=shard2
     hash of h is d482b2d3 high bits=3 shard=shard2
     hash of i is 811a702b high bits=2 shard=shard1
     hash of j is ca745a39 high bits=3 shard=shard2
     hash of k is cfbda5d1 high bits=3 shard=shard2
     hash of l is 1d5d6a2c high bits=0 shard=shard3
     hash of m is 5ae4385c high bits=1 shard=shard4
     hash of n is c651d8ac high bits=3 shard=shard2
     hash of o is 68348473 high bits=1 shard=shard4
     hash of p is 986fdf9a high bits=2 shard=shard1
     hash of q is ff8209e8 high bits=3 shard=shard2
     hash of r is 5c9373f1 high bits=1 shard=shard4
     hash of s is ff4acaf1 high bits=3 shard=shard2
     hash of t is ca87df4d high bits=3 shard=shard2
     hash of u is 62203ae0 high bits=1 shard=shard4
     hash of v is bdafcc55 high bits=2 shard=shard1
     hash of w is ff439d1f high bits=3 shard=shard2
     hash of x is 3e9a9b1b high bits=0 shard=shard3
     hash of y is 477d9216 high bits=1 shard=shard4
     hash of z is c1f69a17 high bits=3 shard=shard2
     ***/
  }

  @Override
  public void doTest() throws Exception {
    boolean testFinished = false;
    try {
      handle.clear();
      handle.put("QTime", SKIPVAL);
      handle.put("timestamp", SKIPVAL);

      // todo: do I have to do this here?
      waitForRecoveriesToFinish(false);

      doHashingTest();
      doTestNumRequests();
      // doAtomicUpdate();  TODO: this currently fails!

      testFinished = true;
    } finally {
      if (!testFinished) {
        printLayoutOnTearDown = true;
      }
    }
  }




  private void doHashingTest() throws Exception {
    log.info("### STARTING doHashingTest");
    assertEquals(4, cloudClient.getZkStateReader().getClusterState().getCollection(DEFAULT_COLLECTION).getSlices().size());
    String shardKeys = ShardParams.SHARD_KEYS;
    // for now,  we know how ranges will be distributed to shards.
    // may have to look it up in clusterstate if that assumption changes.


    doAddDoc("b!doc1");
    doAddDoc("c!doc2");
    doAddDoc("d!doc3");
    doAddDoc("e!doc4");

    doRTG("b!doc1");
    doRTG("c!doc2");
    doRTG("d!doc3");
    doRTG("e!doc4");
    doRTG("b!doc1,c!doc2");
    doRTG("d!doc3,e!doc4");

    commit();

    doQuery("b!doc1,c!doc2,d!doc3,e!doc4", "q","*:*");
    doQuery("b!doc1,c!doc2,d!doc3,e!doc4", "q","*:*", "shards","shard1,shard2,shard3,shard4");
    doQuery("b!doc1,c!doc2,d!doc3,e!doc4", "q","*:*", shardKeys,"b!,c!,d!,e!");
    doQuery("b!doc1", "q","*:*", shardKeys,"b!");
    doQuery("c!doc2", "q","*:*", shardKeys,"c!");
    doQuery("d!doc3", "q","*:*", shardKeys,"d!");
    doQuery("e!doc4", "q","*:*", shardKeys,"e!");

    // try using shards parameter
    doQuery("b!doc1", "q","*:*", "shards",bucket1);
    doQuery("c!doc2", "q","*:*", "shards",bucket2);
    doQuery("d!doc3", "q","*:*", "shards",bucket3);
    doQuery("e!doc4", "q","*:*", "shards",bucket4);


    doQuery("b!doc1,c!doc2", "q","*:*", shardKeys,"b!,c!");
    doQuery("b!doc1,e!doc4", "q","*:*", shardKeys,"b!,e!");

    doQuery("b!doc1,c!doc2", "q","*:*", shardKeys,"b,c");     // query shards that would contain *documents* "b" and "c" (i.e. not prefixes).  The upper bits are the same, so the shards should be the same.

    doQuery("b!doc1,c!doc2", "q","*:*", shardKeys,"b/1!");   // top bit of hash(b)==1, so shard1 and shard2
    doQuery("d!doc3,e!doc4", "q","*:*", shardKeys,"d/1!");   // top bit of hash(b)==0, so shard3 and shard4

    doQuery("b!doc1,c!doc2", "q","*:*", shardKeys,"b!,c!");

    doQuery("b!doc1,c!doc2,d!doc3,e!doc4", "q","*:*", shardKeys,"foo/0!");

    // test targeting deleteByQuery at only certain shards
    doDBQ("*:*", shardKeys,"b!");
    commit();
    doQuery("c!doc2,d!doc3,e!doc4", "q","*:*");
    doAddDoc("b!doc1");

    doDBQ("*:*", shardKeys,"c!");
    commit();
    doQuery("b!doc1,d!doc3,e!doc4", "q","*:*");
    doAddDoc("c!doc2");

    doDBQ("*:*", shardKeys,"c!");
    commit();
    doQuery("b!doc1,d!doc3,e!doc4", "q","*:*");
    doAddDoc("c!doc2");

    doDBQ("*:*", shardKeys,"d!,e!");
    commit();
    doQuery("b!doc1,c!doc2", "q","*:*");
    doAddDoc("d!doc3");
    doAddDoc("e!doc4");

    commit();
  }






  public void doTestNumRequests() throws Exception {
    log.info("### STARTING doTestNumRequests");

    List<CloudJettyRunner> runners = shardToJetty.get(bucket1);
    CloudJettyRunner leader = shardToLeaderJetty.get(bucket1);
    CloudJettyRunner replica =  null;
    for (CloudJettyRunner r : runners) {
      if (r != leader) replica = r;
    }

    long nStart = getNumRequests();
    leader.client.solrClient.add( sdoc("id","b!doc1") );
    long nEnd = getNumRequests();
    assertEquals(2, nEnd - nStart);   // one request to leader, which makes another to a replica


    nStart = getNumRequests();
    replica.client.solrClient.add( sdoc("id","b!doc1") );
    nEnd = getNumRequests();
    assertEquals(3, nEnd - nStart);   // orig request + replica forwards to leader, which forward back to replica.

    nStart = getNumRequests();
    replica.client.solrClient.add( sdoc("id","b!doc1") );
    nEnd = getNumRequests();
    assertEquals(3, nEnd - nStart);   // orig request + replica forwards to leader, which forward back to replica.

    CloudJettyRunner leader2 = shardToLeaderJetty.get(bucket2);


    nStart = getNumRequests();
    replica.client.solrClient.query( params("q","*:*", "shards",bucket1) );
    nEnd = getNumRequests();
    assertEquals(1, nEnd - nStart);   // short circuit should prevent distrib search

    nStart = getNumRequests();
    replica.client.solrClient.query( params("q","*:*", "shard.keys","b!") );
    nEnd = getNumRequests();
    assertEquals(1, nEnd - nStart);   // short circuit should prevent distrib search

    nStart = getNumRequests();
    leader2.client.solrClient.query( params("q","*:*", "shard.keys","b!") );
    nEnd = getNumRequests();
    assertEquals(3, nEnd - nStart);   // original + 2 phase distrib search.  we could improve this!

    nStart = getNumRequests();
    leader2.client.solrClient.query( params("q","*:*") );
    nEnd = getNumRequests();
    assertEquals(9, nEnd - nStart);   // original + 2 phase distrib search * 4 shards.

    nStart = getNumRequests();
    leader2.client.solrClient.query( params("q","*:*", "shard.keys","b!,d!") );
    nEnd = getNumRequests();
    assertEquals(5, nEnd - nStart);   // original + 2 phase distrib search * 2 shards.
  }

  public void doAtomicUpdate() throws Exception {
    log.info("### STARTING doAtomicUpdate");
    int nClients = clients.size();
    assertEquals(8, nClients);

    int expectedVal = 0;
    for (SolrServer client : clients) {
      client.add(sdoc("id", "b!doc", "foo_i", map("inc",1)));
      expectedVal++;

      QueryResponse rsp = client.query(params("qt","/get", "id","b!doc"));
      Object val = ((Map)rsp.getResponse().get("doc")).get("foo_i");
      assertEquals((Integer)expectedVal, val);
    }
  }

    long getNumRequests() {
    long n = controlJetty.getDebugFilter().getTotalRequests();
    for (JettySolrRunner jetty : jettys) {
      n += jetty.getDebugFilter().getTotalRequests();
    }
    return n;
  }


  void doAddDoc(String id) throws Exception {
    index("id",id);
    // todo - target diff servers and use cloud clients as well as non-cloud clients
  }

  // TODO: refactor some of this stuff up into a base class for use by other tests
  void doQuery(String expectedDocs, String... queryParams) throws Exception {
    Set<String> expectedIds = new HashSet<String>( StrUtils.splitSmart(expectedDocs, ",", true) );

    QueryResponse rsp = cloudClient.query(params(queryParams));
    Set<String> obtainedIds = new HashSet<String>();
    for (SolrDocument doc : rsp.getResults()) {
      obtainedIds.add((String) doc.get("id"));
    }

    assertEquals(expectedIds, obtainedIds);
  }

  void doRTG(String ids) throws Exception {
    cloudClient.query(params("qt","/get", "ids",ids));

    Set<String> expectedIds = new HashSet<String>( StrUtils.splitSmart(ids, ",", true) );

    QueryResponse rsp = cloudClient.query(params("qt","/get", "ids",ids));
    Set<String> obtainedIds = new HashSet<String>();
    for (SolrDocument doc : rsp.getResults()) {
      obtainedIds.add((String) doc.get("id"));
    }

    assertEquals(expectedIds, obtainedIds);
  }

  // TODO: refactor some of this stuff into the SolrJ client... it should be easier to use
  void doDBQ(String q, String... reqParams) throws Exception {
    UpdateRequest req = new UpdateRequest();
    req.deleteByQuery(q);
    req.setParams(params(reqParams));
    req.process(cloudClient);
  }

  @Override
  public void tearDown() throws Exception {
    super.tearDown();
  }

}
